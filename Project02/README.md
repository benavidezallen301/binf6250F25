# Introduction
In this project, we explored the Markov model and generating text based off it. With a snippet from *One Fish, Two Fish, Red Fish, Blue Fish* by Dr. Seuss, we trained first Markov model to be memory-less- only taking one word or "state" at a time into account. We then created an *Nth* order Markov chain where the model has a memory of *N* amount of prior states to give our program the capability of generating higher-order models. Finally, we 
created functions to generate randomized text based off of the Markov model created. To ensure functionality, we tested these functions on the entirety of *One Fish, Two Fish, Red Fish, Blue Fish* as well as some Shakespeare. 
# Pseudocode
Put pseudocode in this box:

```
Some pseudocode here
```

# Successes
Description of the team's learning points

# Struggles
Description of the stumbling blocks the team experienced

# Personal Reflections
## Group Leader
Group leader's reflection on the project

## Other member
This project presented great challenges in my knowledge and understanding of data structures, rewarding me with a deeper understanding of dictionary manipulation through the concept of Markov models. Although not applied in this project, I deeply appreciate the application of this model type to omics and cannot wait to continue learning about this and other model types to oen day apply it in a relevant setting. I regret not being able to explore numpy more, as I know a true Markov model utilizes matrices to extract the chances of a future state but I did not utilize numpy's array generation and manipulation capabilities. 

# Generative AI Appendix
As per the syllabus
