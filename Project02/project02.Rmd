---
title: "Project 02"
author: "Jason Bae and Allen Benavidez"
date: "`r Sys.Date()`"
output:
    github_document:
        toc: true
        toc_depth: 2
        html_preview: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library('reticulate')
use_python('/usr/bin/python3')
```

# Markov Chains

---

In class today we will be implementing a Markov chain to process sentences

---

## Learning Objectives

1. Students will be able to explain the Markov Chain process  
1. Implement a Markov Chain

---

## Background

Markov Chains represent a series of events following the Markov Property: future states are memory-less in that they depend only on the current state. This can be expanded to the idea of variable order Markov models where there is a variable-length memory (eg. 1st order Markov Model). Markov models consist of fully observable states.

> A common example of this is in predicting the weather: We can clearly see the current weather and would like to predict tomorrow's weather. This is also applicable to biology with one case being CpG islands.

Our goal today will be to implement a Markov model built from words. For our example text, we will use the classic example of Dr. Seuss because of the repetitive nature of the text.

---

## Train Markov model

For our initial implementation of the Markov Model, we will use the simple example of Dr. Seuss: "One fish two fish red fish blue fish."

```{python}
def build_markov_model(markov_model, new_text):
    '''
    Function to build or add to a 1st order Markov model given a string of text
    We will store the markov model as a dictionary of dictionaries
    The key in the outer dictionary represents the current state
    and the inner dictionary represents the next state with their contents containing
    the transition probabilities.
    Note: This would be easier to read if we were to build a class representation
    of the model rather than a dictionary of dictionaries, but for simplicitiy
    our implementation will just use this structure.
    
    Args: 
    markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)
    new_text (str): a string to build or add to the moarkov_model
    
    Returns:
    markov_model (dict of dicts): an updated markov_model
    
    Pseudocode:
    Add artificial states for start and end
    For each word in text:
    Increment markov_model[word][next_word]
    
    '''
    # Split up the sentence into a list of words
    new_text = new_text.split(" ")
    # Loop through text
    for index, word in enumerate(new_text):
        # Condition to create the starting state
        if index == 0:
            markov_model["*S*"] = {word : 1}
        # Condition for when the loop reaches the last word to not get IndexError
        if inex ==  len(new_text) - 1:
            # Mark end state.
            markov_model[word].update({"*E*" : 1})
            break
        # Condition to add to a pre-existing entry
        if word in markov_model:
            markov_model[word].update({new_text[index+1] : 1})
        else:
            # Create new entry
            markov_model[word] = {new_text[index+1] : 1}
    return markov_model


# Test the function
markov_model = dict()
text = "one fish two fish red fish blue fish"
markov_model = build_markov_model(markov_model, text)
print(markov_model)
```

Expected output:
```
{'*S*': {'one': 1}, 'one': {'fish': 1}, 'fish': {'two': 1, 'red': 1, 'blue': 1, '*E*': 1}, 'two': {'fish': 1}, 'red': {'fish': 1}, 'blue': {'fish': 1}}
```

### Nth order Markov chain

In the above model, each event or word is output from only the previous state with no memory of any prior states. While this is useful in some cases, typical biological applications of Markov chains require higher-order models to accurately capture what we know about a system. For instance, in attempting to identify coding regions of a genome, we know that open reading frames (ORFs) contain codon triplets, and so a third or sixth order Markov chain would better describe these regions. Here you will implement a generalized form of our previous Markov Chain to allow for Nth order chains.

```{python}
def build_markov_model(markov_model, text, order=1):
    '''
    Function to build or add to a Nth order Markov model given a string of text
    
    Args: 
    markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)
    or None if a new model is being built
    new_text (str): a string to build or add to the moarkov_model
    order (int): the number of previous states to consider for the model
    
    Returns:
    markov_model (dict of dicts): an updated/new markov_model
    '''
    new_text = text.split(" ")

    # Condition for if the Markov model is memory-less, same as previous function
    if order == 1:
        for index, word in enumerate(new_text):
            if index == 0:
                markov_model["*S*"] = {word : 1}
            if index ==  len(new_text) - 1:
                if word not in markov_model:
                    markov_model[word] = {}
                markov_model[word].update({"*E*" : 1})
                break
            if word in markov_model:
                markov_model[word].update({new_text[index+1] : 1})
            else:
                markov_model[word] = {new_text[index+1] : 1}
        return markov_model
    
       
    for index, word in enumerate(new_text):
        # State is the "reading frame", joins the requested number of words into one string item
        state = " ".join(new_text[index:index + order])
        if index == 0:
            #consulted openAI chatgpt to know how to use the .setdefault() method
            markov_model.setdefault("*S*",{})
            markov_model["*S*"] = {state : 1}
        # Condition to handle IndexError since it accesses words ahead
        if index ==  len(new_text) - order:
            if state not in markov_model:
                markov_model[state]={}
                markov_model[state]["*E*"] = 1
            break
        # Condition to add to pre-existing entries
        if state in markov_model:
            if new_text[count+order] in markov_model[state]:
                markov_model[state][new_text[index+order]] += 1
            else:
                markov_model[state] = {new_text[index+order]: 1}
        else:
            markov_model[state] = {new_text[index+order] : 1}
    return markov_model

markov_model = dict()
text = "one fish two fish red fish blue fish"
markov_model = build_markov_model(markov_model, text, order=1)
markov_model
```

## Generate text from Markov Model

Markov models are "generative models". That is, the probability states in the model can be used to generate output following the conditional probabilities in the model.

We will now generate a sequence of text from the Markov model. For this section, I recommend using np.random.choice, which allows for you to provide a probability distribution for drawing the next edge in the chain.

```{python}
import numpy as np


def get_next_word(current_word, markov_model, seed=42):
    '''
    Function to randomly move a valid next state given a markov model
    and a current state (word)
    
    Args: 
    current_word (string): a state that exists in our model
    markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)
    seed (int): dictates how numpy should generate random numbers
    
    Returns:
    next_word (str): a randomly selected next word based on transition probabilies
    
    Pseudocode:
    Calculate transition probilities for all next states from a given state (counts/sum)
    Randomly draw from these to generate the next state
    
    '''
    # Access the possible next words given the current state 
    next_states = list(markov_model[current_word])
    # Creates a list of occurrences for each possible next word
    counts = list(markov_model[current_word].values())
    total = sum(counts)
    # Counts/sum.
    # Created lists maintained order, so probabilities should match possible words.
    probabilities = [x/total for x in counts]
    # Uses numpy's random.choice() to randomly select a word based on the probabilities given.
    return np.random.choice(next_states, replace = False, p = probabilities)


def generate_random_text(markov_model, seed=42):
    '''
    Function to generate text given a markov model
    
    Args: 
    markov_model (dict of dicts): a dictionary of word:(next_word:frequency pairs)
    
    Returns:
    sentence (str): a randomly generated sequence given the model
    
    Pseudocode:
    Initialize sentence at start state
    Until End State:
    append get_next_word(current_word, markov_model)
    Return sentence
    
    '''
    # First reading-frame to consider, need to convert to list and then join since the entry is a dictionary
    state = " ".join(list(markov_model["*S*"]))
    # Post-project comment: probably could have made order a function argument
    order = len(state.split(" "))
    # Split string into list again to make it easy to access select words
    sentence = state.split(" ")
    next_word = get_next_word(state, markov_model)
    # While loop to keep generating until End is generated
    while next_word != "*E*":
        # Add next word to the tail of the sentence
        sentence.append(next_word)
        # Create a new state based off the last N amount of words
        state = " ".join(sentence[-order:])
        next_word = get_next_word(state, markov_model)
    # Rejoin list into one string
    sentence = " ".join(sentence)
    return sentence

markov_model = {}
text = "one fish two fish red fish blue fish"
markov_model = build_markov_model(markov_model, text, order = 1)
generate_random_text(markov_model, seed=42)





```

---

## All the Fish

Up till now, you have only been working with a line or two of the Dr. Seuss' _One Fish, Two Fish_. Now, I want you to build a model using the whole book and try different orders of Markov models.

```{python}
# Now just add some more training data to the markov model. You can find it under data/one_fish_two_fish.txt

markov_model = dict()
book = "/courses/BINF6250.202610/students/benavidez.a/Project02/data/one_fish_two_fish.txt"
# Read in the whole book
with open (book,"r") as file:
    text = file.read()
    split = text.split()
    final_text = " ".join(split)

markov_model = build_markov_model(markov_model,final_text , order = 1)
generate_random_text(markov_model, seed=42)
        
pass

print (generate_random_text(markov_model,seed=7))
```

---

## Shakespeare

Now, let's play around with some Shakespeare.

```{python}
# An example of a more complex text that we can use to generate more complex output
sonet_markov_model = dict()
file = open("/courses/BINF6250.202610/students/benavidez.a/Project02/data/sonnets.txt", "r")
    
sonet = ""
for line in file:
    line = line.strip()
    if line == "":
        # Empty line so build model
        sonet_markov_model = build_markov_model(sonet_markov_model, sonet,order=1)
        sonet = ""
    else:
        sonet = sonet + ' ' + line
        
print(generate_random_text(sonet_markov_model,seed=7))
```

